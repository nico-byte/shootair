behaviors:
  Shootair:
    trainer_type:   sac
  hyperparameters:    
    learning_rate:    3e-4
    learning_rate_schedule:   constant
    batch_size:   256
    buffer_size:  58000
    buffer_init_steps:    10000
    tau:  0.005
    steps_per_update: 4.0
    save_replay_buffer:   False
    init_entcoef: 0.5
    reward_signal_steps_per_update:   10.0
  network_settings:   
    normalize:    False
    hidden_units: 256
    num_layers:   1
    vis_encode_type:  simple
    memory:   None
    goal_conditioning_type:   none
  reward_signals: 
    extrinsic:    
      gamma:  0.99
      strength:   1.0
      network_settings:   
        normalize:    False
        hidden_units: 128
        num_layers:   2
        vis_encode_type:  simple
        memory:   None
        goal_conditioning_type:   hyper
    gail: 
      gamma:  0.99
      strength:   0.5
      network_settings:   
        normalize:    False
        hidden_units: 128
        num_layers:   2
        vis_encode_type:  simple
        memory:   None
        goal_conditioning_type:   none
      learning_rate:  3e-4
      encoding_size:  None
      use_actions:    True
      use_vail:   False
      demo_path:  ..................../Assets/Demonstrations
  init_path:  None
  keep_checkpoints:   5
  checkpoint_interval:    500000
  max_steps:  1000000
  time_horizon:   256
  summary_freq:   10000
  threaded:   True
  self_play:  None
  behavioral_cloning: None